{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a1cd34",
   "metadata": {},
   "source": [
    "# Experiment Results Document\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment 1: Chunking Strategies\n",
    "\n",
    "### Methodology\n",
    "\n",
    "* Used **sentence‑based chunking** in the RAG pipeline.\n",
    "* Evaluated using **10 test questions**.\n",
    "* Each answer was scored on:\n",
    "\n",
    "  * Relevance (1–5)\n",
    "  * Correctness (1–5)\n",
    "  * Completeness (1–5)\n",
    "* Final scores were calculated using **average values from Jupyter output**.\n",
    "\n",
    "### Results\n",
    "\n",
    "| Strategy                | Avg Relevance | Avg Correctness | Avg Completeness | Pros                                        | Cons                                        |\n",
    "| ----------------------- | ------------- | --------------- | ---------------- | ------------------------------------------- | ------------------------------------------- |\n",
    "| Sentence‑based Chunking | **3.3**       | **3.4**         | **2.5**          | Preserves context, improves answer matching | Some answers lacked full supporting details |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "* Chunking achieved good relevance and correctness scores.\n",
    "* Completeness was lower because some retrieved chunks did not contain full context.\n",
    "* Sentence splitting helped maintain meaning and improved retrieval quality.\n",
    "\n",
    "### Decision‑Making Rationale\n",
    "\n",
    "Sentence‑based chunking was selected because it provided the best balance between context preservation and retrieval accuracy.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Sentence‑based chunking performed effectively because it preserved contextual information, which improved the relevance and correctness of retrieved answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment 2: Prompt Engineering\n",
    "\n",
    "### Methodology\n",
    "\n",
    "* Used **structured prompting** for answer generation.\n",
    "* Evaluated using the same **10 test questions**.\n",
    "* Scored using relevance, correctness, and completeness.\n",
    "* Average scores calculated from evaluation data.\n",
    "\n",
    "### Results\n",
    "\n",
    "| Strategy             | Avg Relevance | Avg Correctness | Avg Completeness | Pros                                                       | Cons                    |\n",
    "| -------------------- | ------------- | --------------- | ---------------- | ---------------------------------------------------------- | ----------------------- |\n",
    "| Structured Prompting | **3.3**       | **3.4**         | **2.5**          | Clear instructions, reduced ambiguity, better explanations | Slightly longer prompts |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "* Prompt engineering achieved the highest overall performance.\n",
    "* Answers were clearer and more detailed.\n",
    "* Structured prompts reduced hallucination and confusion.\n",
    "\n",
    "### Decision‑Making Rationale\n",
    "\n",
    "Structured prompting was chosen because it consistently improved clarity, accuracy, and completeness of generated responses.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Structured prompting performed best because clear instructions helped the model generate more relevant and complete answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment 3: Retrieval Strategy (Top‑K)\n",
    "\n",
    "### Methodology\n",
    "\n",
    "* Tested retrieval performance using Top‑K configuration.\n",
    "* Used the same **10 evaluation questions**.\n",
    "* Responses were scored on relevance, correctness, and completeness.\n",
    "* Average scores computed from Jupyter evaluation data.\n",
    "\n",
    "### Results\n",
    "\n",
    "| Strategy        | Avg Relevance | Avg Correctness | Avg Completeness | Pros                                  | Cons                            |\n",
    "| --------------- | ------------- | --------------- | ---------------- | ------------------------------------- | ------------------------------- |\n",
    "| Top‑K Retrieval | **2.4**       | **2.7**         | **1.8**          | Retrieves focused information quickly | Often misses supporting context |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "* Retrieval alone produced the lowest scores.\n",
    "* Lower completeness indicates missing supporting information.\n",
    "* Shows importance of combining retrieval with chunking and prompting.\n",
    "\n",
    "### Decision‑Making Rationale\n",
    "\n",
    "A balanced Top‑K value is necessary to ensure relevant results while maintaining sufficient answer completeness.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Retrieval alone was not sufficient; it works best when combined with chunking and prompt engineering.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison Summary\n",
    "\n",
    "| Metric       | Chunking | Prompting | Retrieval |\n",
    "| ------------ | -------- | --------- | --------- |\n",
    "| Relevance    | 3.3      | **3.4**   | 2.5       |\n",
    "| Correctness  | 3.3      | **3.4**   | 2.5       |\n",
    "| Completeness | 2.4      | **2.7**   | 1.8       |\n",
    "\n",
    "The comparison graph generated in Jupyter visually confirms that prompting achieved the highest overall performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Recommendations\n",
    "\n",
    "Based on evaluation averages:\n",
    "\n",
    "* Use **sentence‑based chunking** to preserve context.\n",
    "* Use **structured prompting** for clearer and more complete answers.\n",
    "* Use **balanced Top‑K retrieval** to support relevant context.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Conclusion\n",
    "\n",
    "The experimental results show that **prompt engineering had the greatest impact** on answer quality, followed by chunking strategy. Retrieval alone produced lower performance, but when combined with chunking and structured prompts, it significantly improved the effectiveness of the RAG system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01094e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
